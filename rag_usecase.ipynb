{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Medical Question Answering system using LangChain and Mistral 7B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following block to install required libraries \n",
    "\"\"\"\n",
    "!pip install langchain chromadb sentence-transformers\n",
    "!pip install  openai tiktoken\n",
    "!pip install jq\n",
    "!pip install faiss\n",
    "!pip install pymilvus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting the API key of HuggingFace to load the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']='YOUR_HF_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the PubMed articles from the JSON file. To prepare the JSON file, please refer to the script `download_pubmed.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8267 pubmed articles are loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='Fluoroquinolones (FQs) are one of the most commonly prescribed classes of antibiotics. Although they were initially well tolerated in randomized clinical trials, subsequent epidemiological studies have reported an increased risk of threatening, severe, long-lasting, disabling and irreversible adverse effects (AEs), related to neurotoxicity and collagen degradation, such as tendonitis, Achilles tendon rupture, aortic aneurysm, and retinal detachment. This article reviews the main potentially threatening AEs, the alarms issued by regulatory agencies and therapeutic alternatives.', metadata={'source': '/data/pubmed_article_december-2023.json', 'seq_num': 2, 'year': '2023', 'month': '12', 'day': '22', 'title': 'Safety of fluoroquinolones.'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # Define the metadata extraction function.\n",
    "    metadata[\"year\"] = record.get(\"pub_date\").get('year')\n",
    "    metadata[\"month\"] = record.get(\"pub_date\").get('month')\n",
    "    metadata[\"day\"] = record.get(\"pub_date\").get('day')\n",
    "    metadata[\"title\"] = record.get(\"article_title\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='/data/pubmed_article_december-2023.json',\n",
    "    jq_schema='.[]',\n",
    "    content_key='article_abstract',\n",
    "    metadata_func=metadata_func)\n",
    "data = loader.load()\n",
    "print(f\"{len(data)} pubmed articles are loaded!\")\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chunk abstracts into small text passages for efficient retrieval and LLM context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8267 pubmed articles are converted to 31506 text fragments!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Gait disorders are a common feature of neurological disease. The gait examination is an essential part of the neurological clinical assessment, providing valuable clues to a myriad of causes. Understanding how to examine gait is not only essential for neurological diagnosis but also for treatment and prognosis. Here, we review aspects of the clinical history and examination of neurological gait to help guide gait disorder assessment. We focus particularly on how to differentiate between common gait abnormalities and highlight the characteristic features of the more prevalent neurological gait patterns such as ataxia, waddling, steppage, spastic gait, Parkinson's disease and functional g\", metadata={'source': '/data/pubmed_article_december-2023.json', 'seq_num': 1, 'year': '2023', 'month': '12', 'day': '22', 'title': 'Neurological gait assessment.'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter,CharacterTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=64)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"{len(data)} pubmed articles are converted to {len(chunks)} text fragments!\")\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the embedding model. The following code defines two options for loading the model: \n",
    "    - **Option a:** Using SentenceTransformerEmbeddings framework to load their most performing model `all-mpnet-base-v2`\n",
    "    - **Option b:** Using HuggingFaceEmbeddings hub to load the popular model `e5-large-unsupervised`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option a: using all-mpnet from SentenceTransformer \n",
    "#from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Option b: using e5-large-unspupervised from huggingface \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "modelPath = \"intfloat/e5-large-unsupervised\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name = modelPath,  \n",
    "  model_kwargs = {'device':'cuda'},\n",
    "  encode_kwargs={'normalize_embeddings':False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build the vector databse (VDB) to index the text chunks and their corresponsding vectors. We also define three options to define the VDB: \n",
    "    - **Option a:** Using chromaDB\n",
    "    - **Option b:** Using Milvus\n",
    "    - **Option c:** Using FAISS index\n",
    "\n",
    "#TODO Add definition and comparison between the two options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Using Milvus database\\n# To run the following code, you should have a milvus instance up and running\\n# Follow the instructions in the following the link: https://milvus.io/docs/install_standalone-docker.md\\nfrom langchain.vectorstores import Milvus\\ndb = Milvus.from_documents(\\n    chunks,\\n    embeddings,\\n    connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"},\\n)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Option a: Using chroma database\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(chunks, embeddings)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Option b: Using Milvus database\n",
    "# To run the following code, you should have a milvus instance up and running\n",
    "# Follow the instructions in the following the link: https://milvus.io/docs/install_standalone-docker.md\n",
    "from langchain.vectorstores import Milvus\n",
    "db = Milvus.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"},\n",
    ")\n",
    "'''\n",
    "\n",
    "# Using faiss index\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load pre-trained Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba558485bc24aadb58bc18b0344c7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=False, device_map='auto')\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline = pipe,\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 1024}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the RAG pipeline using LangChain. The LLM's answer highly depends on the prompt template, that's why we tested three different prompts. The one giving the best answer as PROMPT2. \n",
    "\n",
    "#TODO: Add explanation about the three prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time\n",
    "\n",
    "# PROMPT 1\n",
    "PROMPT_TEMPLATE_1 = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "You are allowed to rephrase the answer based on the context. \n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT1 = PromptTemplate.from_template(PROMPT_TEMPLATE_1)\n",
    "\n",
    "# PROMPT 2\n",
    "PROMPT_TEMPLATE_2=\"Your are a medical assistant for question-answering tasks. Answer the Question using the provided Contex only. Your answer should be in your own words and be no longer than 128 words. \\n\\n Context: {context} \\n\\n Question: {question} \\n\\n Answer:\"\n",
    "PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "# PROMPT 3\n",
    "from langchain import hub\n",
    "PROMPT3 = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")\n",
    "\n",
    "# RAG pipeline\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(k=2),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT2},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run one sample query `\"What are the safest cryopreservation methods?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6.445125579833984 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "query = \"What are the safest cryopreservation methods?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(f\"\\n--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The safest cryopreservation methods are those that use ionic liquids, deep eutectic solvents, or certain polymers, which open the door to new cryopreservation methods and are also less toxic to frozen samples.\n",
      "\n",
      "\n",
      "The provided answer is based on the following PubMed articles:\t\n",
      "\t-Advances in Cryopreservatives: Exploring Safer Alternatives.\n",
      "\t-In Vitro and In Silico Antioxidant Activity and Molecular Characterization of Bauhinia ungulata Essential Oil.\n",
      "\t-A new apotirucallane-type protolimonoid from the leaves of <i>Paramignya trimera</i>.\n"
     ]
    }
   ],
   "source": [
    "print(result['result'].strip())\n",
    "titles = ['\\t-'+doc.metadata['title'] for doc in result['source_documents']]\n",
    "print(\"\\n\\nThe provided answer is based on the following PubMed articles:\\t\")\n",
    "print(\"\\n\".join(set(titles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the answer to the sample query from the LLM only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6.110211133956909 seconds ---\n",
      "User 1: I'm not sure what you mean by safest. \n",
      "\n",
      "If you mean safest for the patient, then I would say that the safest method is to not be cryopreserved at all. \n",
      "\n",
      "If you mean safest for the cryopreservation process, then I would say that the safest method is to use a method that has been proven to work.\n",
      "User 0: I mean safest for the patient.\n",
      "User 1: Then I would say that the safest method is to not be cryopreserved at all.\n",
      "User 0:\n"
     ]
    }
   ],
   "source": [
    "# Define the langchain pipeline for llm only\n",
    "from langchain.prompts import PromptTemplate\n",
    "PROMPT_TEMPLATE =\"\"\"Answer the given Question only. Your answer should be in your own words and be no longer than 100 words. \\n\\n Question: {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "llm_chain = PROMPT | llm\n",
    "start_time = time.time()\n",
    "result = llm_chain.invoke({\"question\": query})\n",
    "print(f\"\\n--- {time.time() - start_time} seconds ---\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
